[info | PowerModels]: Suppressing information and warning messages for the rest of this session.  Use the Memento package for more fine-grained control of logging.
[ Info: @NamedTuple{bus::Vector{@NamedTuple{i::Int64, uid::String, v_min::Float64, v_max::Float64}}, shunt::Vector{@NamedTuple{j::Int64, j_sh::Int64, uid::String, bus::Int64, g_sh::Float64, b_sh::Float64}}, acl_branch::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_ln::Int64, uid::String, to_bus::Int64, fr_bus::Int64, c_su::Float64, c_sd::Float64, s_max::Float64, g_sr::Float64, b_sr::Float64, b_ch::Float64, g_fr::Float64, g_to::Float64, b_fr::Float64, b_to::Float64}}, acx_branch::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, uid::String, to_bus::Int64, fr_bus::Int64, c_su::Float64, c_sd::Float64, s_max::Float64, g_sr::Float64, b_sr::Float64, b_ch::Float64, g_fr::Int64, g_to::Int64, b_fr::Int64, b_to::Int64}}, vpd::Vector{@NamedTuple{j::Int64}}, fpd::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, phi_o::Float64}}, vwr::Vector{@NamedTuple{j::Int64}}, fwr::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, tau_o::Float64}}, dc_branch::Vector{@NamedTuple{j::Int64, j_dc::Int64, uid::String, pdc_max::Float64, qdc_fr_min::Float64, qdc_to_min::Float64, qdc_fr_max::Float64, qdc_to_max::Float64, to_bus::Int64, fr_bus::Int64}}, prod::Vector{@NamedTuple{j::Int64, j_pr::Int64, j_prcs::Int64, bus::Int64, uid::String, c_on::Float64, c_su::Float64, c_sd::Float64, p_ru::Float64, p_rd::Float64, p_ru_su::Float64, p_rd_sd::Float64, c_rgu::Vector{Float64}, c_rgd::Vector{Float64}, c_scr::Vector{Float64}, c_nsc::Vector{Float64}, c_rru_on::Vector{Float64}, c_rru_off::Vector{Float64}, c_rrd_on::Vector{Float64}, c_rrd_off::Vector{Float64}, c_qru::Vector{Float64}, c_qrd::Vector{Float64}, p_rgu_max::Float64, p_rgd_max::Float64, p_scr_max::Float64, p_nsc_max::Float64, p_rru_on_max::Float64, p_rru_off_max::Float64, p_rrd_on_max::Float64, p_rrd_off_max::Float64, p_0::Float64, q_0::Float64, p_max::Vector{Float64}, p_min::Vector{Float64}, q_max::Vector{Float64}, q_min::Vector{Float64}, sus::Vector{Vector{Float64}}}}, cons::Vector{@NamedTuple{j::Int64, j_cs::Int64, j_prcs::Int64, bus::Int64, uid::String, c_on::Float64, c_su::Float64, c_sd::Float64, p_ru::Float64, p_rd::Float64, p_ru_su::Float64, p_rd_sd::Float64, c_rgu::Vector{Float64}, c_rgd::Vector{Float64}, c_scr::Vector{Float64}, c_nsc::Vector{Float64}, c_rru_on::Vector{Float64}, c_rru_off::Vector{Float64}, c_rrd_on::Vector{Float64}, c_rrd_off::Vector{Float64}, c_qru::Vector{Float64}, c_qrd::Vector{Float64}, p_rgu_max::Float64, p_rgd_max::Float64, p_scr_max::Float64, p_nsc_max::Float64, p_rru_on_max::Float64, p_rru_off_max::Float64, p_rrd_on_max::Float64, p_rrd_off_max::Float64, p_0::Float64, q_0::Float64, p_max::Vector{Float64}, p_min::Vector{Float64}, q_max::Vector{Float64}, q_min::Vector{Float64}, sus::Vector{Vector{Float64}}}}, active_reserve::Vector{@NamedTuple{n::Int64, n_p::Int64, uid::String, c_rgu::Float64, c_rgd::Float64, c_scr::Float64, c_nsc::Float64, c_rru::Float64, c_rrd::Float64, σ_rgu::Float64, σ_rgd::Float64, σ_scr::Float64, σ_nsc::Float64, p_rru_min::Vector{Float64}, p_rrd_min::Vector{Float64}}}, reactive_reserve::Vector{@NamedTuple{n::Int64, n_q::Int64, uid::String, c_qru::Float64, c_qrd::Float64, q_qru_min::Vector{Float64}, q_qrd_min::Vector{Float64}}}, active_reserve_set_pr::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_p::Int64, j_pr::Int64, j_prcs::Int64}}, active_reserve_set_cs::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_p::Int64, j_cs::Int64, j_prcs::Int64}}, reactive_reserve_set_pr::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_q::Int64, j_pr::Int64, j_prcs::Int64}}, reactive_reserve_set_cs::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_q::Int64, j_cs::Int64, j_prcs::Int64}}}
[ Info: parsed data
[ Info: @NamedTuple{bus::Vector{@NamedTuple{i::Int64, uid::String, v_min::Float64, v_max::Float64}}, shunt::Vector{@NamedTuple{j::Int64, j_sh::Int64, uid::String, bus::Int64, g_sh::Float64, b_sh::Float64}}, acl_branch::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_ln::Int64, uid::String, to_bus::Int64, fr_bus::Int64, c_su::Float64, c_sd::Float64, s_max::Float64, g_sr::Float64, b_sr::Float64, b_ch::Float64, g_fr::Float64, g_to::Float64, b_fr::Float64, b_to::Float64}}, acx_branch::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, uid::String, to_bus::Int64, fr_bus::Int64, c_su::Float64, c_sd::Float64, s_max::Float64, g_sr::Float64, b_sr::Float64, b_ch::Float64, g_fr::Int64, g_to::Int64, b_fr::Int64, b_to::Int64}}, vpd::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, phi_min::Float64, phi_max::Float64}}, fpd::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, phi_o::Float64}}, vwr::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, tau_min::Float64, tau_max::Float64}}, fwr::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, tau_o::Float64}}, dc_branch::Vector{@NamedTuple{j::Int64, j_dc::Int64, uid::String, pdc_max::Float64, qdc_fr_min::Float64, qdc_to_min::Float64, qdc_fr_max::Float64, qdc_to_max::Float64, to_bus::Int64, fr_bus::Int64}}, prod::Vector{@NamedTuple{j::Int64, j_pr::Int64, j_prcs::Int64, bus::Int64, uid::String, c_on::Float64, c_su::Float64, c_sd::Float64, p_ru::Float64, p_rd::Float64, p_ru_su::Float64, p_rd_sd::Float64, c_rgu::Vector{Float64}, c_rgd::Vector{Float64}, c_scr::Vector{Float64}, c_nsc::Vector{Float64}, c_rru_on::Vector{Float64}, c_rru_off::Vector{Float64}, c_rrd_on::Vector{Float64}, c_rrd_off::Vector{Float64}, c_qru::Vector{Float64}, c_qrd::Vector{Float64}, p_rgu_max::Float64, p_rgd_max::Float64, p_scr_max::Float64, p_nsc_max::Float64, p_rru_on_max::Float64, p_rru_off_max::Float64, p_rrd_on_max::Float64, p_rrd_off_max::Float64, p_0::Float64, q_0::Float64, p_max::Vector{Float64}, p_min::Vector{Float64}, q_max::Vector{Float64}, q_min::Vector{Float64}, sus::Vector{Vector{Float64}}}}, cons::Vector{@NamedTuple{j::Int64, j_cs::Int64, j_prcs::Int64, bus::Int64, uid::String, c_on::Float64, c_su::Float64, c_sd::Float64, p_ru::Float64, p_rd::Float64, p_ru_su::Float64, p_rd_sd::Float64, c_rgu::Vector{Float64}, c_rgd::Vector{Float64}, c_scr::Vector{Float64}, c_nsc::Vector{Float64}, c_rru_on::Vector{Float64}, c_rru_off::Vector{Float64}, c_rrd_on::Vector{Float64}, c_rrd_off::Vector{Float64}, c_qru::Vector{Float64}, c_qrd::Vector{Float64}, p_rgu_max::Float64, p_rgd_max::Float64, p_scr_max::Float64, p_nsc_max::Float64, p_rru_on_max::Float64, p_rru_off_max::Float64, p_rrd_on_max::Float64, p_rrd_off_max::Float64, p_0::Float64, q_0::Float64, p_max::Vector{Float64}, p_min::Vector{Float64}, q_max::Vector{Float64}, q_min::Vector{Float64}, sus::Vector{Vector{Float64}}}}, active_reserve::Vector{@NamedTuple{n::Int64, n_p::Int64, uid::String, c_rgu::Float64, c_rgd::Float64, c_scr::Float64, c_nsc::Float64, c_rru::Float64, c_rrd::Float64, σ_rgu::Float64, σ_rgd::Float64, σ_scr::Float64, σ_nsc::Float64, p_rru_min::Vector{Float64}, p_rrd_min::Vector{Float64}}}, reactive_reserve::Vector{@NamedTuple{n::Int64, n_q::Int64, uid::String, c_qru::Float64, c_qrd::Float64, q_qru_min::Vector{Float64}, q_qrd_min::Vector{Float64}}}, active_reserve_set_pr::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_p::Int64, j_pr::Int64, j_prcs::Int64}}, active_reserve_set_cs::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_p::Int64, j_cs::Int64, j_prcs::Int64}}, reactive_reserve_set_pr::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_q::Int64, j_pr::Int64, j_prcs::Int64}}, reactive_reserve_set_cs::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_q::Int64, j_cs::Int64, j_prcs::Int64}}}
[ Info: parsed data
┌ Warning: GPU OOM on this problem, skipping...
└ @ Main ~/ExaModelsPower.jl/benchmarking/benchmark/benchmark_opf.jl:1094
[ Info: @NamedTuple{bus::Vector{@NamedTuple{i::Int64, uid::String, v_min::Float64, v_max::Float64}}, shunt::Vector{@NamedTuple{j::Int64, j_sh::Int64, uid::String, bus::Int64, g_sh::Float64, b_sh::Float64}}, acl_branch::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_ln::Int64, uid::String, to_bus::Int64, fr_bus::Int64, c_su::Float64, c_sd::Float64, s_max::Float64, g_sr::Float64, b_sr::Float64, b_ch::Float64, g_fr::Float64, g_to::Float64, b_fr::Float64, b_to::Float64}}, acx_branch::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, uid::String, to_bus::Int64, fr_bus::Int64, c_su::Float64, c_sd::Float64, s_max::Float64, g_sr::Float64, b_sr::Float64, b_ch::Float64, g_fr::Int64, g_to::Int64, b_fr::Int64, b_to::Int64}}, vpd::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, phi_min::Float64, phi_max::Float64}}, fpd::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, phi_o::Float64}}, vwr::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, tau_min::Float64, tau_max::Float64}}, fwr::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, tau_o::Float64}}, dc_branch::Vector{@NamedTuple{j::Int64, j_dc::Int64, uid::String, pdc_max::Float64, qdc_fr_min::Float64, qdc_to_min::Float64, qdc_fr_max::Float64, qdc_to_max::Float64, to_bus::Int64, fr_bus::Int64}}, prod::Vector{@NamedTuple{j::Int64, j_pr::Int64, j_prcs::Int64, bus::Int64, uid::String, c_on::Float64, c_su::Float64, c_sd::Float64, p_ru::Float64, p_rd::Float64, p_ru_su::Float64, p_rd_sd::Float64, c_rgu::Vector{Float64}, c_rgd::Vector{Float64}, c_scr::Vector{Float64}, c_nsc::Vector{Float64}, c_rru_on::Vector{Float64}, c_rru_off::Vector{Float64}, c_rrd_on::Vector{Float64}, c_rrd_off::Vector{Float64}, c_qru::Vector{Float64}, c_qrd::Vector{Float64}, p_rgu_max::Float64, p_rgd_max::Float64, p_scr_max::Float64, p_nsc_max::Float64, p_rru_on_max::Float64, p_rru_off_max::Float64, p_rrd_on_max::Float64, p_rrd_off_max::Float64, p_0::Float64, q_0::Float64, p_max::Vector{Float64}, p_min::Vector{Float64}, q_max::Vector{Float64}, q_min::Vector{Float64}, sus::Vector{Vector{Float64}}}}, cons::Vector{@NamedTuple{j::Int64, j_cs::Int64, j_prcs::Int64, bus::Int64, uid::String, c_on::Float64, c_su::Float64, c_sd::Float64, p_ru::Float64, p_rd::Float64, p_ru_su::Float64, p_rd_sd::Float64, c_rgu::Vector{Float64}, c_rgd::Vector{Float64}, c_scr::Vector{Float64}, c_nsc::Vector{Float64}, c_rru_on::Vector{Float64}, c_rru_off::Vector{Float64}, c_rrd_on::Vector{Float64}, c_rrd_off::Vector{Float64}, c_qru::Vector{Float64}, c_qrd::Vector{Float64}, p_rgu_max::Float64, p_rgd_max::Float64, p_scr_max::Float64, p_nsc_max::Float64, p_rru_on_max::Float64, p_rru_off_max::Float64, p_rrd_on_max::Float64, p_rrd_off_max::Float64, p_0::Float64, q_0::Float64, p_max::Vector{Float64}, p_min::Vector{Float64}, q_max::Vector{Float64}, q_min::Vector{Float64}, sus::Vector{Vector{Float64}}}}, active_reserve::Vector{@NamedTuple{n::Int64, n_p::Int64, uid::String, c_rgu::Float64, c_rgd::Float64, c_scr::Float64, c_nsc::Float64, c_rru::Float64, c_rrd::Float64, σ_rgu::Float64, σ_rgd::Float64, σ_scr::Float64, σ_nsc::Float64, p_rru_min::Vector{Float64}, p_rrd_min::Vector{Float64}}}, reactive_reserve::Vector{@NamedTuple{n::Int64, n_q::Int64, uid::String, c_qru::Float64, c_qrd::Float64, q_qru_min::Vector{Float64}, q_qrd_min::Vector{Float64}}}, active_reserve_set_pr::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_p::Int64, j_pr::Int64, j_prcs::Int64}}, active_reserve_set_cs::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_p::Int64, j_cs::Int64, j_prcs::Int64}}, reactive_reserve_set_pr::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_q::Int64, j_pr::Int64, j_prcs::Int64}}, reactive_reserve_set_cs::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_q::Int64, j_cs::Int64, j_prcs::Int64}}}
[ Info: parsed data
┌ Warning: GPU OOM on this problem, skipping...
└ @ Main ~/ExaModelsPower.jl/benchmarking/benchmark/benchmark_opf.jl:1131
[ Info: @NamedTuple{bus::Vector{@NamedTuple{i::Int64, uid::String, v_min::Float64, v_max::Float64}}, shunt::Vector{@NamedTuple{j::Int64, j_sh::Int64, uid::String, bus::Int64, g_sh::Float64, b_sh::Float64}}, acl_branch::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_ln::Int64, uid::String, to_bus::Int64, fr_bus::Int64, c_su::Float64, c_sd::Float64, s_max::Float64, g_sr::Float64, b_sr::Float64, b_ch::Float64, g_fr::Float64, g_to::Float64, b_fr::Float64, b_to::Float64}}, acx_branch::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, uid::String, to_bus::Int64, fr_bus::Int64, c_su::Float64, c_sd::Float64, s_max::Float64, g_sr::Float64, b_sr::Float64, b_ch::Float64, g_fr::Int64, g_to::Int64, b_fr::Int64, b_to::Int64}}, vpd::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, phi_min::Float64, phi_max::Float64}}, fpd::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, phi_o::Float64}}, vwr::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, tau_min::Float64, tau_max::Float64}}, fwr::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, tau_o::Float64}}, dc_branch::Vector{@NamedTuple{j::Int64, j_dc::Int64, uid::String, pdc_max::Float64, qdc_fr_min::Float64, qdc_to_min::Float64, qdc_fr_max::Float64, qdc_to_max::Float64, to_bus::Int64, fr_bus::Int64}}, prod::Vector{@NamedTuple{j::Int64, j_pr::Int64, j_prcs::Int64, bus::Int64, uid::String, c_on::Float64, c_su::Float64, c_sd::Float64, p_ru::Float64, p_rd::Float64, p_ru_su::Float64, p_rd_sd::Float64, c_rgu::Vector{Float64}, c_rgd::Vector{Float64}, c_scr::Vector{Float64}, c_nsc::Vector{Float64}, c_rru_on::Vector{Float64}, c_rru_off::Vector{Float64}, c_rrd_on::Vector{Float64}, c_rrd_off::Vector{Float64}, c_qru::Vector{Float64}, c_qrd::Vector{Float64}, p_rgu_max::Float64, p_rgd_max::Float64, p_scr_max::Float64, p_nsc_max::Float64, p_rru_on_max::Float64, p_rru_off_max::Float64, p_rrd_on_max::Float64, p_rrd_off_max::Float64, p_0::Float64, q_0::Float64, p_max::Vector{Float64}, p_min::Vector{Float64}, q_max::Vector{Float64}, q_min::Vector{Float64}, sus::Vector{Vector{Float64}}}}, cons::Vector{@NamedTuple{j::Int64, j_cs::Int64, j_prcs::Int64, bus::Int64, uid::String, c_on::Float64, c_su::Float64, c_sd::Float64, p_ru::Float64, p_rd::Float64, p_ru_su::Float64, p_rd_sd::Float64, c_rgu::Vector{Float64}, c_rgd::Vector{Float64}, c_scr::Vector{Float64}, c_nsc::Vector{Float64}, c_rru_on::Vector{Float64}, c_rru_off::Vector{Float64}, c_rrd_on::Vector{Float64}, c_rrd_off::Vector{Float64}, c_qru::Vector{Float64}, c_qrd::Vector{Float64}, p_rgu_max::Float64, p_rgd_max::Float64, p_scr_max::Float64, p_nsc_max::Float64, p_rru_on_max::Float64, p_rru_off_max::Float64, p_rrd_on_max::Float64, p_rrd_off_max::Float64, p_0::Float64, q_0::Float64, p_max::Vector{Float64}, p_min::Vector{Float64}, q_max::Vector{Float64}, q_min::Vector{Float64}, sus::Vector{Vector{Float64}}}}, active_reserve::Vector{@NamedTuple{n::Int64, n_p::Int64, uid::String, c_rgu::Float64, c_rgd::Float64, c_scr::Float64, c_nsc::Float64, c_rru::Float64, c_rrd::Float64, σ_rgu::Float64, σ_rgd::Float64, σ_scr::Float64, σ_nsc::Float64, p_rru_min::Vector{Float64}, p_rrd_min::Vector{Float64}}}, reactive_reserve::Vector{@NamedTuple{n::Int64, n_q::Int64, uid::String, c_qru::Float64, c_qrd::Float64, q_qru_min::Vector{Float64}, q_qrd_min::Vector{Float64}}}, active_reserve_set_pr::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_p::Int64, j_pr::Int64, j_prcs::Int64}}, active_reserve_set_cs::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_p::Int64, j_cs::Int64, j_prcs::Int64}}, reactive_reserve_set_pr::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_q::Int64, j_pr::Int64, j_prcs::Int64}}, reactive_reserve_set_cs::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_q::Int64, j_cs::Int64, j_prcs::Int64}}}
[ Info: parsed data
┌ Warning: GPU OOM on this problem, skipping...
└ @ Main ~/ExaModelsPower.jl/benchmarking/benchmark/benchmark_opf.jl:1162
[ Info: @NamedTuple{bus::Vector{@NamedTuple{i::Int64, uid::String, v_min::Float64, v_max::Float64}}, shunt::Vector{@NamedTuple{j::Int64, j_sh::Int64, uid::String, bus::Int64, g_sh::Float64, b_sh::Float64}}, acl_branch::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_ln::Int64, uid::String, to_bus::Int64, fr_bus::Int64, c_su::Float64, c_sd::Float64, s_max::Float64, g_sr::Float64, b_sr::Float64, b_ch::Float64, g_fr::Float64, g_to::Float64, b_fr::Float64, b_to::Float64}}, acx_branch::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, uid::String, to_bus::Int64, fr_bus::Int64, c_su::Float64, c_sd::Float64, s_max::Float64, g_sr::Float64, b_sr::Float64, b_ch::Float64, g_fr::Int64, g_to::Int64, b_fr::Int64, b_to::Int64}}, vpd::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, phi_min::Float64, phi_max::Float64}}, fpd::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, phi_o::Float64}}, vwr::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, tau_min::Float64, tau_max::Float64}}, fwr::Vector{@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, tau_o::Float64}}, dc_branch::Vector{@NamedTuple{j::Int64, j_dc::Int64, uid::String, pdc_max::Float64, qdc_fr_min::Float64, qdc_to_min::Float64, qdc_fr_max::Float64, qdc_to_max::Float64, to_bus::Int64, fr_bus::Int64}}, prod::Vector{@NamedTuple{j::Int64, j_pr::Int64, j_prcs::Int64, bus::Int64, uid::String, c_on::Float64, c_su::Float64, c_sd::Float64, p_ru::Float64, p_rd::Float64, p_ru_su::Float64, p_rd_sd::Float64, c_rgu::Vector{Float64}, c_rgd::Vector{Float64}, c_scr::Vector{Float64}, c_nsc::Vector{Float64}, c_rru_on::Vector{Float64}, c_rru_off::Vector{Float64}, c_rrd_on::Vector{Float64}, c_rrd_off::Vector{Float64}, c_qru::Vector{Float64}, c_qrd::Vector{Float64}, p_rgu_max::Float64, p_rgd_max::Float64, p_scr_max::Float64, p_nsc_max::Float64, p_rru_on_max::Float64, p_rru_off_max::Float64, p_rrd_on_max::Float64, p_rrd_off_max::Float64, p_0::Float64, q_0::Float64, p_max::Vector{Float64}, p_min::Vector{Float64}, q_max::Vector{Float64}, q_min::Vector{Float64}, sus::Vector{Vector{Float64}}}}, cons::Vector{@NamedTuple{j::Int64, j_cs::Int64, j_prcs::Int64, bus::Int64, uid::String, c_on::Float64, c_su::Float64, c_sd::Float64, p_ru::Float64, p_rd::Float64, p_ru_su::Float64, p_rd_sd::Float64, c_rgu::Vector{Float64}, c_rgd::Vector{Float64}, c_scr::Vector{Float64}, c_nsc::Vector{Float64}, c_rru_on::Vector{Float64}, c_rru_off::Vector{Float64}, c_rrd_on::Vector{Float64}, c_rrd_off::Vector{Float64}, c_qru::Vector{Float64}, c_qrd::Vector{Float64}, p_rgu_max::Float64, p_rgd_max::Float64, p_scr_max::Float64, p_nsc_max::Float64, p_rru_on_max::Float64, p_rru_off_max::Float64, p_rrd_on_max::Float64, p_rrd_off_max::Float64, p_0::Float64, q_0::Float64, p_max::Vector{Float64}, p_min::Vector{Float64}, q_max::Vector{Float64}, q_min::Vector{Float64}, sus::Vector{Vector{Float64}}}}, active_reserve::Vector{@NamedTuple{n::Int64, n_p::Int64, uid::String, c_rgu::Float64, c_rgd::Float64, c_scr::Float64, c_nsc::Float64, c_rru::Float64, c_rrd::Float64, σ_rgu::Float64, σ_rgd::Float64, σ_scr::Float64, σ_nsc::Float64, p_rru_min::Vector{Float64}, p_rrd_min::Vector{Float64}}}, reactive_reserve::Vector{@NamedTuple{n::Int64, n_q::Int64, uid::String, c_qru::Float64, c_qrd::Float64, q_qru_min::Vector{Float64}, q_qrd_min::Vector{Float64}}}, active_reserve_set_pr::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_p::Int64, j_pr::Int64, j_prcs::Int64}}, active_reserve_set_cs::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_p::Int64, j_cs::Int64, j_prcs::Int64}}, reactive_reserve_set_pr::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_q::Int64, j_pr::Int64, j_prcs::Int64}}, reactive_reserve_set_cs::Vector{@NamedTuple{i::Int64, j::Int64, n::Int64, n_q::Int64, j_cs::Int64, j_prcs::Int64}}}
[ Info: parsed data
ERROR: LoadError: Out of GPU memory trying to allocate 508.143 MiB
Effective GPU memory usage: 99.93% (31.707 GiB/31.728 GiB)
Memory pool usage: 29.859 GiB (31.344 GiB reserved)

Stacktrace:
  [1] _pool_alloc
    @ ~/.julia/packages/CUDA/ja0IX/src/memory.jl:666 [inlined]
  [2] macro expansion
    @ ~/.julia/packages/CUDA/ja0IX/src/memory.jl:623 [inlined]
  [3] macro expansion
    @ ./timing.jl:395 [inlined]
  [4] pool_alloc
    @ ~/.julia/packages/CUDA/ja0IX/src/memory.jl:622 [inlined]
  [5] (::CUDA.var"#1173#1174"{Float64, CuArray{Float64, 1, CUDA.DeviceMemory}, Int64, Int64})()
    @ CUDA ~/.julia/packages/CUDA/ja0IX/src/array.jl:901
  [6] #context!#1025
    @ ~/.julia/packages/CUDA/ja0IX/lib/cudadrv/state.jl:168 [inlined]
  [7] context!
    @ ~/.julia/packages/CUDA/ja0IX/lib/cudadrv/state.jl:163 [inlined]
  [8] resize!(A::CuArray{Float64, 1, CUDA.DeviceMemory}, n::Int64)
    @ CUDA ~/.julia/packages/CUDA/ja0IX/src/array.jl:900
  [9] append!(backend::CUDABackend, a::CuArray{Float64, 1, CUDA.DeviceMemory}, b::Int64, lb::Int64)
    @ ExaModels ~/.julia/packages/ExaModels/DLDmb/src/nlp.jl:333
 [10] variable(::ExaCore{Float64, CuArray{Float64, 1, CUDA.DeviceMemory}, CUDABackend}, ::Int64, ::Vararg{Int64}; start::Int64, lvar::Float64, uvar::Float64)
    @ ExaModels ~/.julia/packages/ExaModels/DLDmb/src/nlp.jl:386
 [11] scopf_model(filename::String, uc_filename::String; backend::CUDABackend, T::Type, include_ctg::Bool, result_set::Vector{Any}, kwargs::@Kwargs{})
    @ ExaModelsPower ~/ExaModelsPower.jl/benchmarking/src/scopf.jl:182
 [12] scopf_model
    @ ~/ExaModelsPower.jl/benchmarking/src/scopf.jl:3 [inlined]
 [13] solve_benchmark_cases(cases::Vector{Tuple{String, String}}, tol::Float64, hardware::String; coords::String, case_style::String, curve::Vector{Float64}, mp::Bool, storage::Bool, sc::Bool, corrective_action_ratio::Float64, include_ctg::Bool)
    @ Main ~/ExaModelsPower.jl/benchmarking/benchmark/benchmark_opf.jl:1081
 [14] top-level scope
    @ ~/ExaModelsPower.jl/benchmarking/benchmark/run_sc_gpu.jl:4
in expression starting at /home/sanjayjo/ExaModelsPower.jl/benchmarking/benchmark/run_sc_gpu.jl:4
Using device: CuDevice(0)
Found existing results at saved_raw_data/benchmark_results_scopf_GPU_default_tol_1e4.csv
@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, phi_min::Float64, phi_max::Float64, t::Int64}[]
This is MadNLP version v0.8.10, running with cuDSS v0.6.0

Number of nonzeros in constraint Jacobian............:   466744
Number of nonzeros in Lagrangian Hessian.............:   161982

Total number of variables............................:   140544
                     variables with only lower bounds:    38880
                variables with lower and upper bounds:    14922
                     variables with only upper bounds:        0
Total number of equality constraints.................:    80406
Total number of inequality constraints...............:    71838
        inequality constraints with only lower bounds:    19242
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:    52596

iter    objective    inf_pr   inf_du inf_compl lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
   0  0.0000000e+00 2.78e+02 1.00e+00 5.62e+02  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1 -2.3209060e+01 2.78e+02 1.18e+00 5.62e+02  -1.0 1.00e+04  -4.0 6.10e-04 4.00e-07h  1
   2 -1.8376644e+05 2.77e+02 1.48e+00 5.61e+02  -1.0 3.00e+04  -4.5 1.26e-04 1.22e-03h  1
   3 -3.6685007e+05 2.77e+02 1.57e+00 5.61e+02  -1.0 9.00e+04  -5.0 3.40e-04 4.64e-04h  1

Number of Iterations....: 3

                                   (scaled)                 (unscaled)
Objective...............:  -3.6685007304551546e+05   -3.6685007304551546e+05
Dual infeasibility......:   1.5684452995558908e+00    1.5684452995558908e+00
Constraint violation....:   2.7721388209457422e+02    2.7721388209457422e+02
Complementarity.........:   5.6103886369875784e+02    5.6103886369875784e+02
Overall NLP error.......:   5.6103886369875784e+02    5.6103886369875784e+02

Number of objective function evaluations             = 4
Number of objective gradient evaluations             = 4
Number of constraint evaluations                     = 4
Number of constraint Jacobian evaluations            = 4
Number of Lagrangian Hessian evaluations             = 3
Total wall-clock secs in solver (w/o fun. eval./lin. alg.)  = 233.451
Total wall-clock secs in linear solver                      =  4.234
Total wall-clock secs in NLP function evaluations           = 30.247
Total wall-clock secs                                       = 267.932

EXIT: Maximum Number of Iterations Exceeded.
MadNCL algorithm

Total number of variables............................:      140544
Total number of constraints..........................:      152244

outer  inner     objective    inf_pr   inf_du    η        μ       ρ 
    0      0 +0.0000000e+00 0.00e+00 0.00e+00 1.00e-01 1.0e-01 1.00e+02
iter    objective    inf_pr   inf_du inf_compl lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
   0  0.0000000e+00 1.00e+06 1.00e+00 5.62e+02  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1 -1.2262538e+05 9.98e+05 2.50e+00 5.61e+02  -1.0 1.00e+04  -4.0 5.74e-04 2.00e-03h  1
   2 -5.0207739e+05 9.96e+05 3.05e+00 5.59e+02  -1.0 3.00e+04  -4.5 4.80e-04 2.35e-03h  1
   3 -5.8130858e+05 9.95e+05 2.89e+00 5.59e+02  -1.0 9.00e+04  -5.0 5.23e-04 1.89e-04h  1
This is MadNLP version v0.8.10, running with cuDSS v0.6.0

Number of nonzeros in constraint Jacobian............:   466744
Number of nonzeros in Lagrangian Hessian.............:   161982

Total number of variables............................:   139502
                     variables with only lower bounds:    38880
                variables with lower and upper bounds:    13880
                     variables with only upper bounds:        0
Total number of equality constraints.................:    80406
Total number of inequality constraints...............:    71838
        inequality constraints with only lower bounds:    19242
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:    52596

iter    objective    inf_pr   inf_du inf_compl lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
   0  0.0000000e+00 2.78e+02 1.00e+00 5.62e+02  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0
   1 -1.9493328e+05 2.77e+02 4.54e+00 5.60e+02  -1.0 1.00e+04  -4.0 6.09e-04 3.36e-03h  1
   2 -5.1795674e+05 2.76e+02 4.96e+00 5.59e+02  -1.0 3.00e+04  -4.5 5.38e-04 2.08e-03h  1
   3 -6.0179823e+05 2.76e+02 4.59e+00 5.59e+02  -1.0 9.00e+04  -5.0 7.71e-04 2.06e-04h  1

Number of Iterations....: 3

                                   (scaled)                 (unscaled)
Objective...............:  -6.0179823402930796e+05   -6.0179823402930796e+05
Dual infeasibility......:   4.5944174024256421e+00    4.5944174024256421e+00
Constraint violation....:   2.7608732585869797e+02    2.7608732585869797e+02
Complementarity.........:   5.5873398227208997e+02    5.5873398227208997e+02
Overall NLP error.......:   5.5873398227208997e+02    5.5873398227208997e+02

Number of objective function evaluations             = 4
Number of objective gradient evaluations             = 4
Number of constraint evaluations                     = 4
Number of constraint Jacobian evaluations            = 4
Number of Lagrangian Hessian evaluations             = 3
Total wall-clock secs in solver (w/o fun. eval./lin. alg.)  = 50.116
Total wall-clock secs in linear solver                      =  2.735
Total wall-clock secs in NLP function evaluations           =  0.357
Total wall-clock secs                                       = 53.207

EXIT: Maximum Number of Iterations Exceeded.
Already have 4 cases stored.
("data/C3E4N00073D1_scenario_303.json", "data/C3E4N00073D1_scenario_303_solution.json")
Skipping C3E4N00073D1_scenario_303 (already in results)
("data/C3E4N00073D2_scenario_303.json", "data/C3E4N00073D2_scenario_303_solution.json")
Skipping C3E4N00073D2_scenario_303 (already in results)
("data/C3E4N00073D3_scenario_303.json", "data/C3E4N00073D3_scenario_303_solution.json")
Skipping C3E4N00073D3_scenario_303 (already in results)
("data/C3E4N00073D2_scenario_911.json", "data/C3E4N00073D2_scenario_911_solution.json")
Skipping C3E4N00073D2_scenario_911 (already in results)
("data/C3E4N00617D1_scenario_002.json", "data/C3E4N00617D1_scenario_002_solution.json")
@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, phi_min::Float64, phi_max::Float64, t::Int64}[(j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 1) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 2) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 3) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 4) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 5) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 6) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 7) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 8) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 9) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 10) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 11) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 12) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 13) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 14) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 15) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 16) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 17) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 18); (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 1) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 2) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 3) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 4) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 5) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 6) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 7) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 8) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 9) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 10) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 11) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 12) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 13) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 14) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 15) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 16) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 17) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 18)]
lifted
Effective GPU memory usage: 31.48% (9.988 GiB/31.728 GiB)
Memory pool usage: 8.447 GiB (9.625 GiB reserved)
@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, phi_min::Float64, phi_max::Float64, t::Int64}[(j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 1) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 2) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 3) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 4) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 5) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 6) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 7) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 8) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 9) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 10) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 11) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 12) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 13) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 14) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 15) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 16) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 17) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 18); (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 1) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 2) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 3) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 4) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 5) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 6) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 7) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 8) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 9) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 10) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 11) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 12) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 13) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 14) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 15) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 16) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 17) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 18)]
hybrid
Effective GPU memory usage: 57.88% (18.363 GiB/31.728 GiB)
Memory pool usage: 16.893 GiB (18.000 GiB reserved)
@NamedTuple{j::Int64, j_ac::Int64, j_xf::Int64, phi_min::Float64, phi_max::Float64, t::Int64}[(j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 1) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 2) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 3) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 4) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 5) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 6) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 7) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 8) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 9) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 10) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 11) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 12) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 13) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 14) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 15) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 16) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 17) (j = 730, j_ac = 730, j_xf = 7, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 18); (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 1) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 2) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 3) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 4) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 5) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 6) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 7) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 8) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 9) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 10) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 11) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 12) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 13) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 14) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 15) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 16) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 17) (j = 849, j_ac = 849, j_xf = 126, phi_min = -0.5235987755982988, phi_max = 0.5235987755982988, t = 18)]
madncl
Effective GPU memory usage: 84.76% (26.894 GiB/31.728 GiB)
Memory pool usage: 25.340 GiB (26.531 GiB reserved)
("data/C3E4N00617D2_scenario_002.json", "data/C3E4N00617D2_scenario_002_solution.json")
